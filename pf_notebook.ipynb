{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-06T17:49:25.483114Z",
     "iopub.status.busy": "2020-10-06T17:49:25.482292Z",
     "iopub.status.idle": "2020-10-06T17:49:30.642342Z",
     "shell.execute_reply": "2020-10-06T17:49:30.640916Z"
    },
    "papermill": {
     "duration": 5.175471,
     "end_time": "2020-10-06T17:49:30.642469",
     "exception": false,
     "start_time": "2020-10-06T17:49:25.466998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Data and ML\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_io as tfio\n",
    "from cloud_tpu_client import Client\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "import pandas as pd\n",
    "\n",
    "# Image preprocessing\n",
    "import pydicom\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Miscellaneous\n",
    "from typing import Tuple, List, Callable\n",
    "import os\n",
    "import multiprocessing\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T17:49:30.668592Z",
     "iopub.status.busy": "2020-10-06T17:49:30.666821Z",
     "iopub.status.idle": "2020-10-06T17:49:30.669240Z",
     "shell.execute_reply": "2020-10-06T17:49:30.669710Z"
    },
    "papermill": {
     "duration": 0.01893,
     "end_time": "2020-10-06T17:49:30.669827",
     "exception": false,
     "start_time": "2020-10-06T17:49:30.650897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = r\"/kaggle/input/osic-pulmonary-fibrosis-progression\"\n",
    "PATIENT_DIR = \"test\"\n",
    "\n",
    "def pathfinder():\n",
    "    paths_to_search = [[\"/kaggle\"]]\n",
    "    \n",
    "    while paths_to_search:\n",
    "        p = paths_to_search.pop(0)\n",
    "        for f in os.listdir(os.path.join(*p)):\n",
    "            if \".dcm\" in f:\n",
    "                return os.path.join(*p[:-2]), p[-2]\n",
    "            new_p = p + [f]\n",
    "            if os.path.isdir(os.path.join(*new_p)):\n",
    "                paths_to_search.append(new_p)\n",
    "                \n",
    "    raise Exception(\"NOT FOUND!\")\n",
    "    \n",
    "    \n",
    "# DATA_DIR, PATIENT_DIR = pathfinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T17:49:30.689927Z",
     "iopub.status.busy": "2020-10-06T17:49:30.689210Z",
     "iopub.status.idle": "2020-10-06T17:49:30.693122Z",
     "shell.execute_reply": "2020-10-06T17:49:30.692620Z"
    },
    "papermill": {
     "duration": 0.015586,
     "end_time": "2020-10-06T17:49:30.693214",
     "exception": false,
     "start_time": "2020-10-06T17:49:30.677628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGET_IMAGE_SHAPE = (224, 224, 3)\n",
    "OUTPUT_SHAPE = (1)\n",
    "META_SHAPE = (10,)\n",
    "QUOTA = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-06T17:49:30.728016Z",
     "iopub.status.busy": "2020-10-06T17:49:30.727155Z",
     "iopub.status.idle": "2020-10-06T17:49:30.729895Z",
     "shell.execute_reply": "2020-10-06T17:49:30.729371Z"
    },
    "papermill": {
     "duration": 0.028886,
     "end_time": "2020-10-06T17:49:30.729987",
     "exception": false,
     "start_time": "2020-10-06T17:49:30.701101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clip_border(image: np.ndarray) -> np.ndarray:\n",
    "    bounds = np.array(np.nonzero(~(image == 0)))\n",
    "    tl = np.min(bounds, axis=1)\n",
    "    br = np.max(bounds, axis=1)\n",
    "\n",
    "    return image[tl[0]:br[0], tl[1]:br[1]]\n",
    "\n",
    "\n",
    "def rescale_for_lungs(image: np.ndarray, meta_image: pydicom.FileDataset, scan_range: str = \"none\") -> np.ndarray:\n",
    "    hounsfield_units = image * meta_image.RescaleSlope + meta_image.RescaleIntercept\n",
    "    if scan_range == \"low\":\n",
    "        lung_min = -1400\n",
    "        lung_max = -950\n",
    "    elif scan_range == \"high\":\n",
    "        lung_min = -240\n",
    "        lung_max = -160\n",
    "    elif scan_range == \"lung\":\n",
    "        lung_min = -1400\n",
    "        lung_max = -200\n",
    "    else:\n",
    "        return hounsfield_units\n",
    "    return (hounsfield_units - lung_min) / (lung_max - lung_min)\n",
    "\n",
    "\n",
    "def map_to_unit(image: np.ndarray) -> np.ndarray:\n",
    "    min_val, max_val = np.min(image), np.max(image)\n",
    "    return (image - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def unified_tone_map(image: np.ndarray) -> np.ndarray:\n",
    "    rescale = 0.1\n",
    "    offset = 0.2\n",
    "    return image * rescale + offset\n",
    "\n",
    "\n",
    "def preprocess(medical_image: pydicom.FileDataset, apply_tone_map=False, scan_range: str = \"none\") -> np.ndarray:\n",
    "    processed = rescale_for_lungs(clip_border(medical_image.pixel_array), medical_image, scan_range)\n",
    "    if apply_tone_map:\n",
    "        return unified_tone_map(processed)\n",
    "    else:\n",
    "        return processed\n",
    "\n",
    "\n",
    "def preprocess_tri_channel(medical_image: pydicom.FileDataset, apply_tone_map=False) -> np.ndarray:\n",
    "    processed = np.stack((\n",
    "        preprocess(medical_image, False, \"low\"),\n",
    "        preprocess(medical_image, False, \"high\"),\n",
    "        preprocess(medical_image, False, \"lung\")\n",
    "    ), axis=2)\n",
    "\n",
    "    if apply_tone_map:\n",
    "        return unified_tone_map(processed)\n",
    "    else:\n",
    "        return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T17:49:30.754416Z",
     "iopub.status.busy": "2020-10-06T17:49:30.753558Z",
     "iopub.status.idle": "2020-10-06T17:49:30.772335Z",
     "shell.execute_reply": "2020-10-06T17:49:30.771751Z"
    },
    "papermill": {
     "duration": 0.034348,
     "end_time": "2020-10-06T17:49:30.772431",
     "exception": false,
     "start_time": "2020-10-06T17:49:30.738083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "first_readings = df.drop_duplicates([\"Patient\"], keep=\"first\")\n",
    "# remaining_weeks = df[~df.isin(first_readings)].dropna()\n",
    "# remaining_weeks = remaining_weeks.merge(first_readings[[\"Patient\", \"Weeks\", \"FVC\", \"Percent\"]], \n",
    "#                                         left_on=\"Patient\", right_on=\"Patient\", suffixes=(\"\", \"_BASE\"))\n",
    "# remaining_weeks = remaining_weeks.drop(\"Percent\", 1).drop_duplicates([\"Patient\", \"Weeks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T17:49:30.807119Z",
     "iopub.status.busy": "2020-10-06T17:49:30.801837Z",
     "iopub.status.idle": "2020-10-06T17:49:30.818494Z",
     "shell.execute_reply": "2020-10-06T17:49:30.818008Z"
    },
    "papermill": {
     "duration": 0.037866,
     "end_time": "2020-10-06T17:49:30.818608",
     "exception": false,
     "start_time": "2020-10-06T17:49:30.780742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def splitter(l: List[int], quota: int) -> List[List[int]]:\n",
    "    qty = len(l) // quota + (1 if len(l) % quota != 0 else 0)\n",
    "    aux_qty = qty * quota\n",
    "    \n",
    "    frac_ls = [len(l) * i / aux_qty for i in range(aux_qty)]\n",
    "    \n",
    "    subsets = [[] for _ in range(qty)]\n",
    "    \n",
    "    for i, fl in enumerate(frac_ls):\n",
    "        subsets[i % qty].append(l[round(fl)])\n",
    "        \n",
    "    return subsets\n",
    "\n",
    "def extract_meta(patient):\n",
    "    # Base FVC: continous (1)\n",
    "    # Healthy FVC: continous (1)\n",
    "    # Base week: continous (1)\n",
    "    # Age: continous (1)\n",
    "    # Gender: categorical (2)\n",
    "    # Smoking status: categorical (3)\n",
    "    # Prediction Week: continous (1)\n",
    "    # Total: 10\n",
    "    \n",
    "    row = first_readings[first_readings[\"Patient\"] == patient]\n",
    "    meta = np.zeros(META_SHAPE, dtype=np.float32)\n",
    "    meta[0] = row[\"FVC\"]\n",
    "    meta[1] = 100 * meta[0] / row[\"Percent\"]\n",
    "    meta[2] = row[\"Weeks\"]\n",
    "    meta[3] = row[\"Age\"] # TRANSFORM\n",
    "    if (row[\"Sex\"] == \"Male\").all():\n",
    "        meta[4: 6] = [1, 0]\n",
    "    else:\n",
    "        meta[4: 6] = [0, 1]\n",
    "    status = row[\"SmokingStatus\"]\n",
    "    if (status == \"Ex-smoker\").all():\n",
    "        meta[6:9] = [1, 0, 0]\n",
    "    elif (status == \"Never smoked\").all():\n",
    "        meta[6:9] = [0, 1, 0]\n",
    "    elif (status == \"Currently smokes\").all():\n",
    "        meta[6:9] = [0, 0, 1]\n",
    "    meta[9] = 0\n",
    "    \n",
    "#     fvc = np.zeros([1], dtype=np.float32)\n",
    "#     fvc[0] = row[\"FVC\"]\n",
    "    \n",
    "    return tf.convert_to_tensor(meta, dtype_hint=tf.float32)\n",
    "\n",
    "\n",
    "def load_image_sample(patient, sample_set):\n",
    "    w, h, c = TARGET_IMAGE_SHAPE\n",
    "    images = np.zeros([len(sample_set), w, h, c])\n",
    "#     patient = patient.numpy().decode(\"utf8\")\n",
    "    for i, sample in enumerate(sample_set):\n",
    "        try:\n",
    "            dcm_file = pydicom.dcmread(os.path.join(DATA_DIR, PATIENT_DIR, patient, f\"{sample}.dcm\"))\n",
    "            images[i] = cv2.resize(preprocess_tri_channel(dcm_file, True), (w, h))\n",
    "        except Exception as e:\n",
    "            #print(\"Broken:\", e)\n",
    "            pass\n",
    "    return tf.convert_to_tensor(images, dtype=tf.float32)\n",
    "            \n",
    "            \n",
    "def file_set_generator(quota: int):\n",
    "    TOTAL_PATIENT_SET = os.listdir(os.path.join(DATA_DIR, PATIENT_DIR))\n",
    "    \n",
    "    all_subsets = []\n",
    "\n",
    "    for patient in TOTAL_PATIENT_SET:\n",
    "        files = sorted(map(lambda f: int(f.replace(\".dcm\", \"\")), os.listdir(os.path.join(DATA_DIR, PATIENT_DIR, patient))))\n",
    "        subsets = splitter(files, quota)\n",
    "        for ss in subsets:\n",
    "            all_subsets.append((patient, ss))\n",
    "            \n",
    "#     rand.shuffle(all_subsets)\n",
    "        \n",
    "    for patient, ss in all_subsets:\n",
    "        # Load the image data\n",
    "        image_data = load_image_sample(patient, ss)\n",
    "        meta = extract_meta(patient)\n",
    "        yield (meta, image_data), tf.convert_to_tensor([patient], dtype_hint=tf.string)\n",
    "\n",
    "\n",
    "def test_set_generator():\n",
    "    return file_set_generator(QUOTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T17:49:31.613160Z",
     "iopub.status.busy": "2020-10-06T17:49:31.612218Z",
     "iopub.status.idle": "2020-10-06T17:49:33.457460Z",
     "shell.execute_reply": "2020-10-06T17:49:33.456651Z"
    },
    "papermill": {
     "duration": 2.630583,
     "end_time": "2020-10-06T17:49:33.457594",
     "exception": false,
     "start_time": "2020-10-06T17:49:30.827011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "w, h, c = TARGET_IMAGE_SHAPE\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    test_set_generator,\n",
    "    ((tf.float32, tf.float32), tf.string),\n",
    "    ((tf.TensorShape(META_SHAPE), tf.TensorShape([QUOTA, w, h, c])), tf.TensorShape([1]))\n",
    ").batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T17:49:33.479305Z",
     "iopub.status.busy": "2020-10-06T17:49:33.478657Z",
     "iopub.status.idle": "2020-10-06T17:49:38.411540Z",
     "shell.execute_reply": "2020-10-06T17:49:38.410373Z"
    },
    "papermill": {
     "duration": 4.945221,
     "end_time": "2020-10-06T17:49:38.411662",
     "exception": false,
     "start_time": "2020-10-06T17:49:33.466441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(r\"../input/hopefully-good-model/continue_probably_overfit.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T17:49:38.438857Z",
     "iopub.status.busy": "2020-10-06T17:49:38.437803Z",
     "iopub.status.idle": "2020-10-06T17:49:38.456893Z",
     "shell.execute_reply": "2020-10-06T17:49:38.456392Z"
    },
    "papermill": {
     "duration": 0.035726,
     "end_time": "2020-10-06T17:49:38.457007",
     "exception": false,
     "start_time": "2020-10-06T17:49:38.421281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fixed_model = keras.Model(\n",
    "    inputs=model.input[1],\n",
    "    outputs=model.get_layer(\"lstm_2\").output\n",
    ")\n",
    "dist_model = keras.Model(\n",
    "    inputs=[model.input[0], keras.Input(tensor=fixed_model.output)], \n",
    "    outputs=model.output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T17:49:38.493142Z",
     "iopub.status.busy": "2020-10-06T17:49:38.492195Z",
     "iopub.status.idle": "2020-10-06T17:50:56.585278Z",
     "shell.execute_reply": "2020-10-06T17:50:56.585951Z"
    },
    "papermill": {
     "duration": 78.120126,
     "end_time": "2020-10-06T17:50:56.586175",
     "exception": false,
     "start_time": "2020-10-06T17:49:38.466049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples_per_patient_week = 1\n",
    "seen_patients = []\n",
    "\n",
    "curr_patient = 1\n",
    "\n",
    "last_patient = None\n",
    "\n",
    "last_image = None\n",
    "\n",
    "counter = 0\n",
    "\n",
    "LAST_PATIENT_DATA = []\n",
    "\n",
    "START_WEEK = -12\n",
    "END_WEEK = 133\n",
    "\n",
    "\n",
    "def process_patient(data, patient_name, out_file):\n",
    "    num_subsets = len(data)\n",
    "\n",
    "    data_points = np.zeros([END_WEEK - START_WEEK + 1, samples_per_patient_week], dtype=np.float32)\n",
    "\n",
    "    for images, meta in data:\n",
    "#         base_cnn_calc = fixed_cnn_model(image, training=False)\n",
    "#         base_meta_calc = fixed_meta_model(meta, training=False)\n",
    "        base_calc = fixed_model(image, training=False)\n",
    "    \n",
    "        for week in range(START_WEEK, END_WEEK + 1):\n",
    "            meta[0, 9] = week\n",
    "            for retry in range(samples_per_patient_week):\n",
    "#                 data_points[week, retry] += dist_model((base_cnn_calc, base_meta_calc), training=True) / num_subsets\n",
    "                data_points[week, retry] += dist_model((meta, base_calc), training=True) / num_subsets\n",
    "\n",
    "    week_preds = np.mean(data_points, axis=1)\n",
    "    week_stds = np.ones([data_points.shape[0]]) * 200 # np.std(data_points, axis=1, ddof=1)\n",
    "    \n",
    "    for week, mean, std in zip(range(START_WEEK, END_WEEK + 1), week_preds, week_stds):\n",
    "        out_file.write(f\"{patient_name}_{week},{mean},{std}\\n\")\n",
    "    \n",
    "\n",
    "with open(\"submission.csv\", \"w\") as out_file:\n",
    "    out_file.write(\"Patient_Week,FVC,Confidence\\n\")\n",
    "    \n",
    "    for ((meta, image), patient) in dataset:\n",
    "        patient = patient.numpy()[0, 0].decode(\"utf8\")\n",
    "        \n",
    "        if last_patient is None:\n",
    "            last_patient = patient\n",
    "\n",
    "        patient_data = df[df[\"Patient\"] == patient]\n",
    "\n",
    "        if last_patient != patient:\n",
    "            process_patient(LAST_PATIENT_DATA, last_patient, out_file)\n",
    "            LAST_PATIENT_DATA = []\n",
    "\n",
    "        last_patient = patient\n",
    "\n",
    "        meta_numpy = meta.numpy()\n",
    "\n",
    "        LAST_PATIENT_DATA.append((image.numpy(), meta_numpy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 96.776238,
   "end_time": "2020-10-06T17:50:58.196183",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-06T17:49:21.419945",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
